# -*- coding: utf-8 -*-
"""Source Code 1213R FYP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1svnRiwS0u_-hl1SF3_2iiyhY8HWI_rvD

**IMPORT LIBRARIES**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from plotly.offline import init_notebook_mode
init_notebook_mode(connected=True)

"""**LOAD DATASET**"""

data = pd.read_csv('Mental Health.csv')

data.describe(include='all').T

data.shape

data.duplicated().sum()

"""# **DATA CLEANING**"""

#Rename columns
data.columns = ['Timestamp', 'Age', 'Gender', 'Origin', 'States', 'Uni', 'Year',
                'Course', 'Live_Campus', 'Study_hours', 'Cgpa', 'Extracurricular',
                'Level_academic_stress', 'Ac_stressor', 'Ac_sleep', 'Sleep_hours',
                'Sleep_difficulty', 'DaysFor_PhysicalAct', 'PhysicalAct_barriers',
                'Daily_diet', 'Meal_per_day', 'Smoke', 'Life_event',
                'Life_event_stress', 'Finance_status', 'Finance_stress',
                'Finance_skip_meals', 'Employed', 'Work_balance', 'Work_academic_miss',
                'Mental_status', 'Stress_level', 'Depression_anxiety', 'Social_support',
                'Counsel','Social_media_usage', 'Social_media_mental', 'Screen_time',
                'Screen_time_AMPM', 'Screen_time_wellbeing', 'Economic_concern',
                'PostGrad_plans', 'Plan_mental', 'Career_mental' ]

data.info()

#Drop columns
columns_to_drop = ['Timestamp','Age','Origin', 'Live_Campus', 'Study_hours',
                   'PhysicalAct_barriers', 'Smoke', 'Life_event',
                   'Life_event_stress', 'Employed', 'Work_balance',
                   'Work_academic_miss','Social_media_mental',
                   'Screen_time_AMPM','Screen_time_wellbeing']
data.drop(columns=columns_to_drop, inplace=True)

#Remove other universities
Uni = 'Universiti Islam Antarabangsa Malaysia (UIAM)'

data = data[data['Uni'] == 'Universiti Islam Antarabangsa Malaysia (UIAM)']

#Drop university column
data.drop(columns=['Uni'], inplace=True)

# Convert categorical columns to numerical values
data.replace({"Gender":{'Male':0,'Female':1},
              "Year":{'First year':1,'Second year':2, 'Third year':3, 'Fourth year':4},
              "Cgpa":{'High (GPA between 3.50 and 4.00)':3,
                      'Average (GPA between 3.00 and 3.49)':2,
                      'Low (GPA below 3.00)':1,
                      'I prefer not to disclose or assess my academic performance.':1},
              "Extracurricular":{'No':0,'Yes':1},
              "Level_academic_stress":{'Low':0,'Moderate':1,'High':2, 'Very High':3},
              "Ac_sleep":{'Never':0,'Seldom':1,'Sometimes':2,'Often':3,
                          'Almost always':4},
              "Sleep_hours":{'Less than 6 hours':0,'6-7 hours':1,
                             '7-8 hours':2, 'More than 8 hours':3},
              "Sleep_difficulty":{'No, rarely':0,'Yes, occasionally':1,
                                  'Yes, frequently':2},
              "DaysFor_PhysicalAct":{'None':0,'1-2 days':1,
                                     '3-4 days':2, '5 or more days':3},
              "Daily_diet":{'Inconsistent diet':0,'High in fast food/takeout':1,
                            'High in processed foods/junk foods':2,
                            'Balanced diet including all food groups':3,
                            'Mostly fruits and vegetables':4},
              "Meal_per_day":{'Less than 3':0,'3':1, '4':2, 'More than 4':3},
              "Finance_status":{'Poor':0, 'Fair':1, 'Good':2, 'Excellent':3},
              "Finance_stress":{'Not at all':0,'Slightly':1,'Moderately':2,
                                'Very Much':3, 'Extremely':4},
              "Finance_skip_meals":{'Never':0,'Seldom':1, 'Sometimes':2, 'Often':3},
              "Counsel": {'No':0,'Yes':1},
              "Social_media_usage": {'Never':0,'Rarely':1, 'Once a month':2,
                                     'Once a week':3, 'Several times a week':4,
                                     'Daily':5},
              "Screen_time":{'Less than 1 hour':0,'1 - 2 hours':1, '2 - 4 hours':2,
                             '4 -6 hours':3, 'More than 6 hours':4}
              },inplace=True)

data.info()

data.nunique()

"""**Standardising States and Course**"""

#Show states that are interpreted differently
print(data['States'].unique())
print(data['States'].nunique())

#Change states name into a standard states name
data['States'].replace({'kedah': 'Kedah' , 'Putrajaya':'Wilayah Persekutuan',
                        'Penang ' : 'Pulau Pinang',
                        'Kuala Lumpur':'Wilayah Persekutuan',
                        'kuala lumpur ' : 'Wilayah Persekutuan',
                        'KUALA LUMPUR':'Wilayah Persekutuan',
                        'Selangior':'Selangor', 'PERAK':'Perak' ,
                        'Batu pahat, johor':'Johor',
                        'Wilayah persekutuan':'Wilayah Persekutuan',
                        'Terengganu.':'Terengganu',
                        'KELANTAN ': 'Kelantan', 'perak': 'Perak',
                         'WP Putrajaya ': 'Wilayah Persekutuan',
                        'Shah Alam, Selangor': 'Selangor',
                        'Shah Alam' : 'Selangor',
                        'Negeri Sembilan ' : 'Negeri Sembilan',
                        'Selangor ' : 'Selangor', 'Kelantan ' : 'Kelantan',
                        'selangor ': 'Selangor',
                        'Kuala Kubu Bharu, Selangor': 'Selangor',
                        'Bukit Mertajam, Penang ' : 'Pulau Pinang',
                        'ملاك' : 'Melaka', 'Sarawak.':'Sarawak'
                        } , inplace = True)

#Show unique values of states
states = data['States']
states = states.unique()

# Print unique values
for value in states:
    print(value)

print(data['States'].nunique())

#Remove Bangladesh
condition = (data['States'] != 'Bangladesh')
data = data[condition]  # Keep rows that meet the condition

"""**Feature engineering**"""

region_groups = {
    'Northern Region': ['Kedah', 'Perak', 'Penang', 'Pulau Pinang'],
    'Central Region': ['Selangor', 'Wilayah Persekutuan', 'Negeri Sembilan', 'Melaka'],
    'Southern Region': ['Johor'],
    'Eastern Region': ['Terengganu', 'Kelantan', 'Pahang'],
    'East Malaysia': ['Sarawak', 'Sabah']
}

# Create a new feature 'Region' based on the grouping
data['Region'] = data['States'].apply(lambda x: next((region for region, states in region_groups.items() if x in states), 'Unknown'))

# Convert the 'Region' feature to dummy variables (one-hot encoding)
region_dummies = pd.get_dummies(data['Region'], prefix='Region')

#Show courses that are interpreted differently
course = data['Course']

# Get unique values in the column
course = course.unique()

# Print unique values
for value in course:
    print(value)

print(data['Course'].nunique())

#Change course name into a standard course name

#Engineering
data['Course'].replace({'mechanical-automotive engineering ( honours)': 'Engineering' ,
                        'Bachelor in Communication Engineering ':'Engineering',
                        'Chemical Engineering' : 'Engineering',
                        'Bachelor of Materials Engineering with Honours':'Engineering',
                        'Mechatronics ':'Engineering', 'Mechatronics':'Engineering',
                        'Bachelor in Communication Engineering':'Engineering',
                        'MCT':'Engineering' , 'Beee' : 'Engineering',
                        'Mechatronics engineering ':'Engineering',
                        'اينجينيريڠ' : 'Engineering',
                        'Bachelor of Engineering (Communication)(Hons.)' :'Engineering',
                        'Bachelor of Aerospace Engineering': 'Engineering',
                        'Engineerning' : 'Engineering'
                        } , inplace = True)


#Information and Communication Technology
data['Course'].replace({'Comouter Science':'Information and Communication Technology',
                        'Computer science ' : 'Information and Communication Technology',
                        'Bachelor in computer science' : 'Information and Communication Technology',
                        'Bachelor of Computer Science': 'Information and Communication Technology',
                        'bachelor in information technology' : 'Information and Communication Technology',
                        'BIT' : 'Information and Communication Technology',
                        'Bachelor of Information Technology ': 'Information and Communication Technology',
                        'IT' : 'Information and Communication Technology',
                        'Kulliyah of Information Technology ' : 'Information and Communication Technology',
                        'Information Technology ': 'Information and Communication Technology',
                        'BACHELOR IN COMPUTER SCIENCE (HONOURS)' : 'Information and Communication Technology',
                        'Bachelor of Information Technology' : 'Information and Communication Technology'
                        } , inplace = True)


#Architecture and Environmental Design
data['Course'].replace({'Applied Arts and Design' : 'Architecture',
                        'Landscape Architechture ' : 'Architecture',
                        'Quantity Surveying'  : 'Architecture'
                        } , inplace = True)


#Sustainable Tourism and Contemporary Languages
data['Course'].replace({'Malay communication' : 'Languages',
                        'MALAY COMMUNICATION' : 'Languages',
                        'MCOM' : 'Languages',
                        'Malay Communication' : 'Languages',
                        'BACHELOR OF ARTS IN MALAY LANGUAGE FOR INTERNATIONAL COMMUNICATION' : 'Languages',
                        'ARCOM' : 'Languages',
                        'ARABIC FOR INTERNATIONAL COMMUNICATION' : 'Languages',
                        'Arabic Language for International Communication ' : 'Languages',
                        'Arabic for International Communication' : 'Languages',
                        'Arabic for International Communicatio ': 'Languages',
                        'Barb' : 'Languages',
                        'Arabic Language for International Communication':'Languages',
                        'Arabic Communication':'Languages',
                        'ENGLISH FOR INTERNATIONAL COMMUNICATION' : 'Languages',
                        'English for International Communication' : 'Languages',
                        'Encom' : 'Languages', 'BENL' : 'Languages',
                        'English Communication' : 'Languages',
                        'English for International Communication ' : 'Languages',
                        'BACHELOR OF ARTS IN ENGLISH FOR INTERNATIONAL COMMUNICATION' : 'Languages',
                        'Foundation in English' : 'Languages',
                        'ENCOM' : 'Languages',
                        'Bachelor of Tourism Management' : 'Tourism',
                        'Tourism management ' : 'Tourism',
                        'Bachelor in tourism management' : 'Tourism'
                        } , inplace = True)


#Islamic Revealed Knowledge and Human Sciences
data['Course'].replace({'Human Sciences in Psychology' : 'Human Sciences' ,
                        'Bachelor of Human Sciences in Psychology ' : 'Human Sciences',
                        'Psychology ' : 'Human Sciences',
                        'Human Sciences (Political Science)' : 'Human Sciences',
                        'Bachelor in Political Science': 'Human Sciences',
                        'Political Science' : 'Human Sciences',
                        'Political Sciences' : 'Human Sciences',
                        'SOCIOLOGY AND ANTHROPOLOGY ' : 'Human Sciences',
                        'Bachelor of Human Sciences in English Language and Literature' : 'Human Sciences',
                        'Communication ' : 'Human Sciences',
                        'Bachelor of English Language and Literature' : 'Human Sciences',
                        'Arabic Language and Literature' : 'Islamic Revealed Knowledge',
                        'English language and literature ' : 'Human Sciences',
                        'Quran and sunnah' : 'Islamic Revealed Knowledge',
                        'Quran sunnah' : 'Islamic Revealed Knowledge',
                        'Quran and Sunnah' : 'Islamic Revealed Knowledge'
                        } , inplace = True)

#Education
data['Course'].replace({'Islamic Education, KOED' : 'Education',
                        'islamic education ' : 'Education',
                        'BACHELOR IN EDUCATION IN ISLAMIC EDUCATION (HONOURS)' : 'Education',
                        'Bachelor of education teaching arabic as second language' : 'Education',
                        'TEASL' : 'Education', 'Islamic Education' : 'Education',
                        'Education ' : 'Education',
                        'Bachelor of Education in Islamic Education ' : 'Education'
                        } , inplace = True)

#Allied Health Sciences
data['Course'].replace({'ALLHS':'Allied Health Sciences',
                        'Physiotherapy ' : 'Allied Health Sciences',
                        'Bachelor of Physiotherapy': 'Allied Health Sciences',
                        'Bachelor of Optometry' : 'Allied Health Sciences',
                        'Dietetics' : 'Allied Health Sciences',
                        'Foundation of Allied Health Science' : 'Allied Health Sciences',

                        } , inplace = True)

#Sciences
data['Course'].replace({'Bachelor of Mathematical Sciences (Honours)' : 'Sciences',
                        'Biotechnology' : 'Sciences',
                        'BACHELOR OF MARINE SCIENCE AND OCEANOGRAPHY ' : 'Sciences',
                        'Physics' : 'Sciences',
                        'Bachelor in Science (Physics)' : 'Sciences'
                        } , inplace = True)

#Halal Industry Management
data['Course'].replace({'Bachelor in Halal Industry Management ' : 'Halal Industry Management',
                        'BAHIM' : 'Halal Industry Management'
                        } , inplace = True)

#Law
data['Course'].replace({'LAW' : 'Law',
                        } , inplace = True)

#Nursing
data['Course'].replace({'Bachelor of nursing ' : 'Nursing',
                        'Bachelor of Nursing (Honours)' : 'Nursing'
                        } , inplace = True)

#Medicine
data['Course'].replace({'MBBS' : 'Medicine'
                        } , inplace = True)


#Economics
data['Course'].replace({'Bachelor of Economics' : 'Economics',
                        'econs' : 'Economics',
                        'Islamic Finance' : 'Economics',
                        'Business administration ' : 'Economics',
                        'BBA' : 'Economics',
                        'BACHELOR OF FINANCE (ISLAMIC FINANCE)' : 'Economics',
                        'econ' : 'Economics', 'Business' : 'Economics',
                        'Accounting' : 'Economics', 'accounting' : 'Economics',
                        'Finance' : 'Economics'
                        } , inplace = True)

#Pharmacy
data['Course'].replace({'PHARMACY' : 'Pharmacy'
                        } , inplace = True)

#Show unique values of courses
course = data['Course']
course = course.unique()

for value in course:
    print(value)

print(data['Course'].nunique())

# One-Hot Encode the 'Course' feature
course_dummies = pd.get_dummies(data['Course'], prefix='Course')

data = pd.concat([data, region_dummies, course_dummies], axis=1)

"""**Impute missing data**"""

# Function to create a dataframe with number and percentage of missing data in the dataframe

def missing_to_df(data):
    total_missing_data = data.isnull().sum().sort_values(ascending=False)
    percentage_missing_data = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending=False)
    missing_data = pd.concat([total_missing_data, percentage_missing_data], axis=1, keys=['Total', 'Percent'])
    return missing_data

missing_data = missing_to_df(data)
missing_data[missing_data['Total']>0]

data.isnull().sum()

# Replace "None" with "No factor"
data['Ac_stressor'].replace('None', 'No factor', inplace=True)

# Fill NaN values with "0 days"
data['Ac_stressor'].fillna('No factor', inplace=True)

print(data['Ac_stressor'])

# Replace "None" with "0 days"
data['DaysFor_PhysicalAct'].replace('None', 0, inplace=True)

# Fill NaN values with "0 days"
data['DaysFor_PhysicalAct'].fillna( 0 , inplace=True)

data['DaysFor_PhysicalAct'] = data['DaysFor_PhysicalAct'].astype(int)

print(data['DaysFor_PhysicalAct'])

data['Depression_anxiety'].fillna('No symptoms', inplace=True)

print(data['Depression_anxiety'])

data['Economic_concern'].fillna('None', inplace=True)

print(data['Economic_concern'])

data['PostGrad_plans'].fillna('None', inplace=True)

print(data['PostGrad_plans'])

data.isnull().sum()

data.to_csv('mental_health_new.csv', index=False) #write csv file

"""**Check feature correlation**"""

import pandas as pd

# Select only numeric columns for the correlation matrix
numeric_features = data.select_dtypes(include=[float, int])
# Calculate the Pearson correlation matrix
correlation_matrix = numeric_features.corr(method='pearson')

# Print the correlation matrix
print(correlation_matrix)
print(correlation_matrix)

#Visualize the correlation matrix using a heatmap
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Pearson Correlation Matrix')
plt.show()

"""Features correlated with mental status"""

# Select only numerical features
numerical_features = data.select_dtypes(include=[int, float])

# Calculate the correlation matrix
correlation_matrix = numerical_features.corr()

# Get the correlation of each feature with the target variable 'Mental_status'
correlation_with_target = correlation_matrix["Mental_status"].sort_values(ascending=False)
print("Correlation with target 'Mental_status':")
print(correlation_with_target)

# Select top features highly correlated with 'Mental_status'
top_features = correlation_with_target[correlation_with_target.index != "Mental_status"].head(10)
print("Top features correlated with 'Mental_status':")
print(top_features)

# Get the correlation of each feature with the target variable 'Mental_status'
correlation_with_target = correlation_matrix["Mental_status"].sort_values(ascending=False)
correlation_with_target = correlation_with_target[correlation_with_target.index != "Mental_status"]

plt.figure(figsize=(12, 6))
sns.barplot(x=correlation_with_target.values, y=correlation_with_target.index, palette='viridis')
plt.title("Correlation with Mental_status")
plt.xlabel("Correlation coefficient")
plt.ylabel("Features")
plt.show()

# Filter positive correlations for the correlation matrix
positive_correlation_matrix = correlation_matrix.copy()
positive_correlation_matrix[positive_correlation_matrix < 0] = 0

# Get positive correlations with the target variable 'Mental_status'
positive_correlation_with_target = correlation_with_target[correlation_with_target > 0]

# Visualize positive correlation with target
plt.figure(figsize=(12, 6))
sns.barplot(x=positive_correlation_with_target.values, y=positive_correlation_with_target.index, palette='viridis')
plt.title("Positive Correlations with Mental_status")
plt.xlabel("Correlation coefficient")
plt.ylabel("Features")
plt.show()

"""Features correlated with stress level"""

# Select only numerical features
numerical_features = data.select_dtypes(include=[int, float])

# Calculate the correlation matrix
correlation_matrix = numerical_features.corr()

# Get the correlation of each feature with the target variable 'Mental_status'
correlation_with_target = correlation_matrix["Stress_level"].sort_values(ascending=False)
print("Correlation with target 'Stress level':")
print(correlation_with_target)

# Select top features highly correlated with 'Mental_status'
top_features = correlation_with_target[correlation_with_target.index != "Stress_level"].head(10)
print("Top features correlated with 'Stress level':")
print(top_features)

# Get the correlation of each feature with the target variable 'Mental_status'
correlation_with_target = correlation_matrix["Stress_level"].sort_values(ascending=False)
correlation_with_target = correlation_with_target[correlation_with_target.index != "Stress_level"]

plt.figure(figsize=(12, 6))
sns.barplot(x=correlation_with_target.values, y=correlation_with_target.index, palette='viridis')
plt.title("Correlation with Stress level")
plt.xlabel("Correlation coefficient")
plt.ylabel("Features")
plt.show()

# Filter positive correlations for the correlation matrix
positive_correlation_matrix = correlation_matrix.copy()
positive_correlation_matrix[positive_correlation_matrix < 0] = 0

# Get positive correlations with the target variable 'Stress_level'
positive_correlation_with_target = correlation_with_target[correlation_with_target > 0]

# Visualize positive correlation with target
plt.figure(figsize=(12, 6))
sns.barplot(x=positive_correlation_with_target.values, y=positive_correlation_with_target.index, palette='viridis')
plt.title("Positive Correlations with Stress_level")
plt.xlabel("Correlation coefficient")
plt.ylabel("Features")
plt.show()

"""**Statistics Summary**"""

data.describe().T

data.describe(include='all').T

"""# **DATA VISUALIZATION**"""

#To decode data
reverse_mapping = {
    "Gender": {0: 'Male', 1: 'Female'},
    "Year": {1: 'First year', 2: 'Second year', 3: 'Third year', 4: 'Fourth year'},
    "Cgpa": {3: 'High (GPA between 3.50 and 4.00)',
             2: 'Average (GPA between 3.00 and 3.49)',
             1: 'Low (GPA below 3.00)',
             1: 'I prefer not to disclose or assess my academic performance.'},
    "Extracurricular": {0: 'No', 1: 'Yes'},
    "Level_academic_stress": {0: 'Low', 1: 'Moderate', 2: 'High', 3: 'Very High'},
    "Ac_sleep": {0: 'Never', 1: 'Seldom', 2: 'Sometimes', 3: 'Often', 4: 'Almost always'},
    "Sleep_hours": {0: 'Less than 6 hours', 1: '6-7 hours', 2: '7-8 hours', 3: 'More than 8 hours'},
    "Sleep_difficulty": {0: 'No, rarely', 1: 'Yes, occasionally', 2: 'Yes, frequently'},
    "DaysFor_PhysicalAct": {0: 'None', 1: '1-2 days', 2: '3-4 days', 3: '5 or more days'},
    "Daily_diet": {0: 'Inconsistent diet', 1: 'High in fast food/takeout', 2: 'High in processed foods/junk foods',
                   3: 'Balanced diet including all food groups', 4: 'Mostly fruits and vegetables'},
    "Meal_per_day": {0: 'Less than 3', 1: '3', 2: '4', 3: 'More than 4'},
    "Finance_status": {0: 'Poor', 1: 'Fair', 2: 'Good', 3: 'Excellent'},
    "Finance_stress": {0: 'Not at all', 1: 'Slightly', 2: 'Moderately', 3: 'Very Much', 4: 'Extremely'},
    "Finance_skip_meals": {0: 'Never', 1: 'Seldom', 2: 'Sometimes', 3: 'Often'},
    "Counsel": {0: 'No', 1: 'Yes'},
    "Social_media_usage": {0: 'Never', 1: 'Rarely', 2: 'Once a month', 3: 'Once a week', 4: 'Several times a week', 5: 'Daily'},
    "Screen_time": {0: 'Less than 1 hour', 1: '1 - 2 hours', 2: '2 - 4 hours', 3: '4 - 6 hours', 4: 'More than 6 hours'}
}

#copy of data
data_decode = data.copy()

# Reverse replace numerical values with categorical values
for column, mapping in reverse_mapping.items():
    data_decode[column] = data_decode[column].replace(mapping)

# Select the columns of interest
columns_of_interest = ['Daily_diet', 'Sleep_hours', 'DaysFor_PhysicalAct', 'Mental_status']

# Create a subset of the data with the selected columns
subset_data = data[columns_of_interest]

# Compute the correlation matrix
correlation_matrix = subset_data.corr()

# Create a correlation heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(20, 4))
fig.suptitle('Value Counts of Gender, States, Year, Course')


# Plotting gender distribution
gender_counts = data_decode['Gender'].value_counts()
axs[0].bar(gender_counts.index, gender_counts, color='lightgreen')
axs[0].set_xlabel('Gender')
axs[0].set_ylabel('Number of Respondents')
axs[0].set_title('Gender Distribution Among Students')

# Plotting origin distribution
origin_counts = data['States'].value_counts()
axs[1].bar(origin_counts.index, origin_counts, color=['lightblue', 'lightgreen'])
axs[1].set_xlabel('Origin')
axs[1].set_ylabel('Number of Students')
axs[1].set_title('Distribution of Students')
axs[1].set_xticklabels(origin_counts.index, rotation=90)

# Plotting gender distribution
gender_counts = data_decode['Year'].value_counts()
axs[2].bar(gender_counts.index, gender_counts, color='lightgreen')
axs[2].set_xlabel('Year')
axs[2].set_ylabel('Number of Students')
axs[2].set_title('Distribution of Students')


plt.tight_layout()
plt.show()

# Count the number of students in each course
course_counts = data['Course'].value_counts()

# Plotting the bar graph for course distribution
plt.figure(figsize=(8, 6))
course_counts.plot(kind='bar', color='skyblue')
plt.xlabel('Course')
plt.ylabel('Number of Students')
plt.title('Distribution of Students Across Courses')
plt.xticks(rotation=45, ha='right')
plt.yticks(range(0, course_counts.max() + 1))
plt.tight_layout()
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(40, 8))
fig.suptitle('Value Counts of Sleep, Diet, Exercise')


# Plotting sleep hours distribution
gender_counts = data_decode['Sleep_hours'].value_counts()
axs[0].bar(gender_counts.index, gender_counts, color='lightgreen')
axs[0].set_xlabel('Sleep hours')
axs[0].set_ylabel('Number of Students')
axs[0].set_title('Sleep Hours Among Students')

# Plotting daily diet distribution
origin_counts = data_decode['Daily_diet'].value_counts()
axs[1].bar(origin_counts.index, origin_counts, color=['lightblue', 'lightgreen'])
axs[1].set_xlabel('Daily diet')
axs[1].set_ylabel('Number of Students')
axs[1].set_title('Diet Among Students')
axs[1].set_xticklabels(origin_counts.index, rotation=90)

# Plotting days for physical activity distribution
gender_counts = data_decode['DaysFor_PhysicalAct'].value_counts()
axs[2].bar(gender_counts.index, gender_counts, color='lightgreen')
axs[2].set_xlabel('Days of Physical Activity')
axs[2].set_ylabel('Number of Students')
axs[2].set_title('Physical Activities Students')


plt.tight_layout()
plt.show()

fig, axs = plt.subplots(1, 2, figsize=(20, 4))
fig.suptitle('Value Counts of Sleep, Diet, Exercise')


# Plotting financial status distribution
gender_counts = data_decode['Finance_status'].value_counts()
axs[0].bar(gender_counts.index, gender_counts, color='lightgreen')
axs[0].set_xlabel('Financial status')
axs[0].set_ylabel('Number of Students')
axs[0].set_title('Financial Status Among Students')

# Plotting screen time distribution
origin_counts = data_decode['Screen_time'].value_counts()
axs[1].bar(origin_counts.index, origin_counts, color=['lightblue', 'lightgreen'])
axs[1].set_xlabel('Screen time')
axs[1].set_ylabel('Number of Students')
axs[1].set_title('Screen time Among Students')



plt.tight_layout()
plt.show()

sns.lineplot(x= data.Mental_status,y = data.Stress_level)
plt.show()

"""**Data Science Question**

Question 1: How does stress level vary among different year of students
"""

# Count the number of unique values in the 'Year' column
num_years = len(data['Year'].unique())

# Generate positions for each boxplot
positions = range(1, num_years + 1)

# Box plot to examine academic stress levels across different courses
plt.figure(figsize=(10, 6))
sns.boxplot(x='Year', y='Stress_level', data=data)
plt.title('Academic Stress Levels Across Different Study Year')
plt.xlabel('Year')
plt.xticks(range(num_years), data['Year'].unique())  # Set x-axis ticks to match unique years
plt.show()

"""Question 2: How does the stress level varies to different courses of study?"""

# Box plot to examine academic stress levels across different courses
plt.figure(figsize=(10, 6))
sns.boxplot(x='Course', y='Stress_level', data=data)
plt.title('Academic Stress Levels Across Different Courses')
plt.xlabel('Course')
plt.ylabel('Academic Stress Level')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility
plt.tight_layout()
plt.show()

# Scatter plot for Screen Time vs Academic Stress
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Screen_time', y='Level_academic_stress', data=data, hue='Social_media_usage', palette='Set2')
plt.title('Screen Time vs Academic Stress by Social Media Usage')
plt.xlabel('Screen Time (hours)')
plt.ylabel('Academic Stress Level')
plt.legend(title='Social Media Usage')
plt.tight_layout()
plt.show()

"""**WORD CLOUD**"""

# install wordcloud
!pip install wordcloud

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import re
nltk.download('stopwords')

# Download the punkt tokenizer
nltk.download('punkt')

def clean_text(text):

    # Remove punctuation and numbers
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)

    # Convert to lowercase
    text = text.lower()

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = ' '.join([word for word in text.split() if word not in stop_words])

    return text

def preprocess_text(text, stopwords):
    # Tokenize the text
    tokens = word_tokenize(text)

    # Initialize the PorterStemmer
    stemmer = PorterStemmer()

    # Stem each token and remove stopwords
    #stemmed_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stopwords]
    stemmed_tokens = [stemmer.stem(token) for token in tokens if token.isalnum() and token.lower() not in stopwords]

    # Join the stemmed tokens back into a single string
    return " ".join(stemmed_tokens)

stopwords = set(STOPWORDS)

depression_anxiety_text = preprocess_text(" ".join(data['Depression_anxiety'].astype(str).tolist()), stopwords)
ac_stressor_text = preprocess_text(" ".join(data['Ac_stressor'].astype(str).tolist()), stopwords)
economic_concern_text = preprocess_text(" ".join(data['Economic_concern'].astype(str).tolist()), stopwords)
plan_mental_text = preprocess_text(" ".join(data['Plan_mental'].astype(str).tolist()), stopwords)
career_mental_text = preprocess_text(" ".join(data['Career_mental'].astype(str).tolist()), stopwords)
post_grad_plans_text = preprocess_text(" ".join(data['PostGrad_plans'].astype(str).tolist()), stopwords)

# Create and display word clouds for each preprocessed text

texts = {
    'Depression and Anxiety': depression_anxiety_text,
    'Academic Stressors': ac_stressor_text,
    'Economic Concerns': economic_concern_text,
    'Mental Health Plans': plan_mental_text,
    'Career Mental Plans': career_mental_text,
    'Post-Graduation Plans': post_grad_plans_text
}

for title, text in texts.items():
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        stopwords=stopwords
    ).generate(text)

# Generate and display word clouds individually
for title, text in texts.items():
    # Generate word cloud
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        colormap='viridis',
        stopwords=stopwords
    ).generate(text)

    # Display the word cloud
    plt.figure(figsize=(10, 5))
    plt.title(title)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

# Import the Counter class from the collections module
from collections import Counter

# Function to preprocess text and plot top words
def plot_top_words(text, title):
    # Tokenize the text
    tokens = word_tokenize(text)

    # Calculate word frequencies
    word_frequencies = Counter(tokens)

    # Create DataFrame for word frequencies
    d = pd.DataFrame({'Word': list(word_frequencies.keys()), 'Count': list(word_frequencies.values())})

    # Select top 10 words
    d = d.nlargest(columns='Count', n=10)

    # Plot bar chart
    plt.figure(figsize=(10, 6))
    sns.barplot(data=d, x='Word', y='Count')
    plt.title(f"Top 10 Most Frequent Words in {title}")
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

# Loop through each text category and plot the top 10 words
for title, text in texts.items():
    plot_top_words(text, title)

data.to_csv('mental_health_new_decode.csv', index=False) #write csv file

"""**FEATURE SCALING NEW**"""

X = data.drop('Mental_status', axis=1)
y = data['Mental_status']

columns_to_scale = ["Year", "Cgpa", "Level_academic_stress", "Ac_sleep", "Sleep_hours",
                    "Sleep_difficulty", "DaysFor_PhysicalAct", "Daily_diet", "Meal_per_day",
                    "Finance_status", "Finance_stress", "Finance_skip_meals",
                    "Social_media_usage", "Screen_time"]

from sklearn.preprocessing import StandardScaler, LabelEncoder

scaler = StandardScaler()
X[columns_to_scale] = scaler.fit_transform(X[columns_to_scale])

"""# **MODEL SELECTION**

NEW WITH SELECTED FEATURES

# *Random forest*
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV

# Select final features
selected_features = [
    'Year', 'Gender', 'Stress_level', 'DaysFor_PhysicalAct',
    'Finance_stress', 'Social_support', 'Sleep_difficulty',
] + list(region_dummies.columns) + list(course_dummies.columns)

# Prepare the data
X = data[selected_features]
y = data['Mental_status']

# Split the data into training and testing sets (before SMOTE)
X_train_before, X_test_before, y_train_before, y_test_before = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
rf_before = RandomForestClassifier(random_state=42)

# Hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

grid_search_before = GridSearchCV(estimator=rf_before, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search_before.fit(X_train_before, y_train_before)

# Best parameters before SMOTE
best_params_before = grid_search_before.best_params_
print(f"Best parameters before SMOTE: {best_params_before}")

# Train the model with the best parameters before SMOTE
best_rf_before = grid_search_before.best_estimator_
best_rf_before.fit(X_train_before, y_train_before)

# Make predictions before SMOTE
y_pred_before = best_rf_before.predict(X_test_before)

# Evaluate the model before SMOTE
rf_accuracy_before = accuracy_score(y_test_before, y_pred_before)
print(f"Accuracy before SMOTE: {rf_accuracy_before}")
print(classification_report(y_test_before, y_pred_before))

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate metrics
rf_accuracy_before = accuracy_score(y_test_before, y_pred_before)
rf_precision_before = precision_score(y_test_before, y_pred_before, average='weighted', zero_division=1)
rf_recall_before = recall_score(y_test_before, y_pred_before, average='weighted', zero_division=1)
rf_f1_before = f1_score(y_test_before, y_pred_before, average='weighted', zero_division=1)

# Print feature importances before SMOTE
rf_feature_importances_before = pd.DataFrame(best_rf_before.feature_importances_,
                                          index=selected_features,
                                          columns=['importance']).sort_values('importance', ascending=False)
print("Feature importances before SMOTE:")
print(rf_feature_importances_before)

"""# *SVM*"""

from sklearn.svm import SVC

selected_features = [
    'Year', 'Gender', 'Stress_level', 'DaysFor_PhysicalAct',
    'Finance_stress', 'Social_support', 'Sleep_difficulty',
] + list(region_dummies.columns) + list(course_dummies.columns)

# Prepare the data
X = data[selected_features]
y = data['Mental_status']

# Split the data into training and testing sets (before SMOTE)
X_train_before, X_test_before, y_train_before, y_test_before = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the SVM model
svm_before = SVC(random_state=42)

# Hyperparameter tuning for SVM (before SMOTE)
param_grid_before = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto'],
    'kernel': ['linear', 'rbf', 'poly']
}

grid_search_before = GridSearchCV(estimator=svm_before, param_grid=param_grid_before, cv=3, n_jobs=-1, verbose=2)
grid_search_before.fit(X_train_before, y_train_before)

# Best parameters (before SMOTE)
best_params_before = grid_search_before.best_params_
print(f"Best parameters before SMOTE: {best_params_before}")

# Train the SVM model with the best parameters (before SMOTE)
best_svm_before = grid_search_before.best_estimator_
best_svm_before.fit(X_train_before, y_train_before)

# Make predictions (before SMOTE)
y_pred_before = best_svm_before.predict(X_test_before)

"""SVM before SMOTE"""

# Evaluate the model (before SMOTE)
svm_accuracy_before = accuracy_score(y_test_before, y_pred_before)
print(f"Accuracy before SMOTE: {svm_accuracy_before}")
print(classification_report(y_test_before, y_pred_before))

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate metrics
svm_accuracy_before = accuracy_score(y_test_before, y_pred_before)
svm_precision_before = precision_score(y_test_before, y_pred_before, average='weighted', zero_division=1)
svm_recall_before = recall_score(y_test_before, y_pred_before, average='weighted', zero_division=1)
svm_f1_before = f1_score(y_test_before, y_pred_before, average='weighted', zero_division=1)

"""# *Deep learning (Feed Forward)*"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam, RMSprop
from sklearn.base import BaseEstimator
from sklearn.utils import check_random_state
from keras.utils import to_categorical

# Define a Keras classifier for use with GridSearchCV
class KerasClassifier(BaseEstimator):
    def __init__(self, input_shape, optimizer='adam', batch_size=32, epochs=10, verbose=0):
        self.input_shape = input_shape
        self.optimizer = optimizer
        self.batch_size = batch_size
        self.epochs = epochs
        self.verbose = verbose
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential()
        model.add(Input(shape=self.input_shape))
        model.add(Dense(64, activation='relu'))
        model.add(Dense(32, activation='relu'))
        model.add(Dense(6, activation='softmax'))
        if self.optimizer == 'adam':
            optimizer = Adam()
        elif self.optimizer == 'rmsprop':
            optimizer = RMSprop()
        else:
            raise ValueError("Invalid optimizer.")
        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        return model

    def fit(self, X, y):
        y_categorical = to_categorical(y)  # One-hot encode the target
        self.model.fit(X, y_categorical, batch_size=self.batch_size, epochs=self.epochs, verbose=self.verbose)
        return self

    def predict(self, X):
        return self.model.predict(X).argmax(axis=1)

    def score(self, X, y):
        y_pred = self.predict(X)
        return accuracy_score(y, y_pred)

# Select final features
selected_features = [
    'Year', 'Gender', 'Stress_level', 'DaysFor_PhysicalAct',
    'Finance_stress', 'Social_support', 'Sleep_difficulty',
] + list(region_dummies.columns) + list(course_dummies.columns)

# Convert selected features to float
for col in selected_features:
    data[col] = pd.to_numeric(data[col], errors='coerce')
    data[col] = data[col].astype(float)

# Prepare the data
X = data[selected_features]
y = data['Mental_status']

# Split the data into training and testing sets (before SMOTE)
X_train_before, X_test_before, y_train_before, y_test_before = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Keras classifier
input_shape = (X_train_before.shape[1],)
dl_model_before = KerasClassifier(input_shape=input_shape)

# Define hyperparameters for tuning (before SMOTE)
param_grid_before = {
    'optimizer': ['adam', 'rmsprop'],
    'batch_size': [32, 64, 128],
    'epochs': [10, 20, 30]
}

grid_search_before = GridSearchCV(estimator=dl_model_before, param_grid=param_grid_before, cv=3, verbose=2)
grid_search_before.fit(X_train_before, y_train_before)

# Best parameters (before SMOTE)
best_params_before = grid_search_before.best_params_
print(f"Best parameters before SMOTE: {best_params_before}")

# Initialize the Keras classifier with the best parameters (before SMOTE)
best_dl_model_before = KerasClassifier(input_shape=input_shape, **best_params_before)

# Fit the best model (before SMOTE)
best_dl_model_before.fit(X_train_before, y_train_before)

# Make predictions (before SMOTE)
y_pred_before = best_dl_model_before.predict(X_test_before)

"""Deep learning before SMOTE"""

# Evaluate the model (before SMOTE)
dl_accuracy_before = accuracy_score(y_test_before, y_pred_before)
print(f"Accuracy before SMOTE: {dl_accuracy_before}")
print(classification_report(y_test_before, y_pred_before))

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate metrics
dl_accuracy_before = accuracy_score(y_test_before, y_pred_before)
dl_precision_before = precision_score(y_test_before, y_pred_before, average='weighted', zero_division=1)
dl_recall_before = recall_score(y_test_before, y_pred_before, average='weighted', zero_division=1)
dl_f1_before = f1_score(y_test_before, y_pred_before, average='weighted', zero_division=1)

"""# **SMOTE-TECHNIQUE ON DATASET**

# *Random forest with smote*
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from imblearn.over_sampling import SMOTE

# Select final features
selected_features = [
    'Year', 'Gender', 'Stress_level', 'DaysFor_PhysicalAct',
    'Finance_stress', 'Social_support', 'Sleep_difficulty',
] + list(region_dummies.columns) + list(course_dummies.columns)

# Prepare the data
X = data[selected_features]
y = data['Mental_status']

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Initialize the model (after SMOTE)
rf_after = RandomForestClassifier(random_state=42, class_weight='balanced')

# Hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

grid_search = GridSearchCV(estimator=rf_after, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters after SMOTE
best_params = grid_search.best_params_
print(f"Best parameters after SMOTE: {best_params}")

# Train the model with the best parameters after SMOTE
best_rf = grid_search.best_estimator_
best_rf.fit(X_train, y_train)

# Make predictions after SMOTE
y_pred = best_rf.predict(X_test)

"""Random Forest after SMOTE"""

# Evaluate the model after SMOTE
rf_accuracy_after = accuracy_score(y_test, y_pred)
print(f"Accuracy after SMOTE: {rf_accuracy_after}")
print(classification_report(y_test, y_pred))

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate metrics
rf_accuracy_after = accuracy_score(y_test, y_pred)
rf_precision_after = precision_score(y_test, y_pred, average='weighted', zero_division=1)
rf_recall_after = recall_score(y_test, y_pred, average='weighted', zero_division=1)
rf_f1_after = f1_score(y_test, y_pred, average='weighted', zero_division=1)

# Print feature importances after SMOTE
rf_feature_importances_after = pd.DataFrame(best_rf.feature_importances_,
                                         index=selected_features,
                                         columns=['importance']).sort_values('importance', ascending=False)
print("Feature importances after SMOTE:")
print(rf_feature_importances_after)

"""# SVM with smote"""

selected_features = [
    'Year', 'Gender', 'Stress_level', 'DaysFor_PhysicalAct',
    'Finance_stress', 'Social_support', 'Sleep_difficulty',
] + list(region_dummies.columns) + list(course_dummies.columns)

# Prepare the data
X = data[selected_features]
y = data['Mental_status']

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Initialize the SVM model (after SMOTE)
svm_after = SVC(random_state=42)

# Hyperparameter tuning for SVM (after SMOTE)
param_grid_after = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto'],
    'kernel': ['linear', 'rbf', 'poly']
}

grid_search_after = GridSearchCV(estimator=svm_after, param_grid=param_grid_after, cv=3, n_jobs=-1, verbose=2)
grid_search_after.fit(X_train, y_train)

# Best parameters (after SMOTE)
best_params_after = grid_search_after.best_params_
print(f"Best parameters after SMOTE: {best_params_after}")

# Train the SVM model with the best parameters (after SMOTE)
best_svm_after = grid_search_after.best_estimator_
best_svm_after.fit(X_train, y_train)

# Make predictions (after SMOTE)
y_pred_after = best_svm_after.predict(X_test)

"""SVM after SMOTE"""

# Evaluate the model (after SMOTE)
svm_accuracy_after = accuracy_score(y_test, y_pred_after)
print(f"Accuracy after SMOTE: {svm_accuracy_after}")
print(classification_report(y_test, y_pred_after))

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate metrics
svm_accuracy_after = accuracy_score(y_test, y_pred)
svm_precision_after = precision_score(y_test, y_pred, average='weighted', zero_division=1)
svm_recall_after = recall_score(y_test, y_pred, average='weighted', zero_division=1)
svm_f1_after = f1_score(y_test, y_pred, average='weighted', zero_division=1)

"""# *Deep learning with smote*"""

# Select final features
selected_features = [
    'Year', 'Gender', 'Stress_level', 'DaysFor_PhysicalAct',
    'Finance_stress', 'Social_support', 'Sleep_difficulty',
] + list(region_dummies.columns) + list(course_dummies.columns)

# Convert selected features to float
for col in selected_features:
    data[col] = pd.to_numeric(data[col], errors='coerce')
    data[col] = data[col].astype(float)

# Prepare the data
X = data[selected_features]
y = data['Mental_status']

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split the resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Initialize the Keras classifier (after SMOTE)
input_shape = (X_train.shape[1],)
dl_model_after = KerasClassifier(input_shape=input_shape)

# Define hyperparameters for tuning (after SMOTE)
param_grid_after = {
    'optimizer': ['adam', 'rmsprop'],
    'batch_size': [32, 64, 128],
    'epochs': [10, 20, 30]
}

grid_search_after = GridSearchCV(estimator=dl_model_after, param_grid=param_grid_after, cv=3, verbose=2)
grid_search_after.fit(X_train, y_train)

# Best parameters (after SMOTE)
best_params_after = grid_search_after.best_params_
print(f"Best parameters after SMOTE: {best_params_after}")

# Initialize the Keras classifier with the best parameters (after SMOTE)
best_dl_model_after = KerasClassifier(input_shape=input_shape, **best_params_after)

best_dl_model_after.fit(X_train, y_train)

# Make predictions
y_pred_after = best_dl_model_after.predict(X_test)

"""Deep learning after SMOTE"""

# Evaluate the model
dl_accuracy_after = accuracy_score(y_test, y_pred_after)
print(f"Accuracy after SMOTE: {dl_accuracy_after}")
print(classification_report(y_test, y_pred_after))

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate metrics
dl_accuracy_after = accuracy_score(y_test, y_pred_after)
dl_precision_after = precision_score(y_test, y_pred_after, average='weighted', zero_division=1)
dl_recall_after = recall_score(y_test, y_pred_after, average='weighted', zero_division=1)
dl_f1_after = f1_score(y_test, y_pred_after, average='weighted', zero_division=1)

"""# **SUMMARY**"""

# Collect performance metrics
metrics = {
    'RF_with_SMOTE': {'accuracy': rf_accuracy_after, 'precision': rf_precision_after, 'recall': rf_recall_after, 'f1': rf_f1_after},
    'RF_without_SMOTE': {'accuracy': rf_accuracy_before, 'precision': rf_precision_before, 'recall': rf_recall_before, 'f1': rf_f1_before},
    'SVM_with_SMOTE': {'accuracy': svm_accuracy_after, 'precision': svm_precision_after, 'recall': svm_recall_after, 'f1': svm_f1_after},
    'SVM_without_SMOTE': {'accuracy': svm_accuracy_before, 'precision': svm_precision_before, 'recall': svm_recall_before, 'f1': svm_f1_before},
    'DL_with_SMOTE': {'accuracy': dl_accuracy_after, 'precision': dl_precision_after, 'recall': dl_recall_after, 'f1': dl_f1_after},
    'DL_without_SMOTE': {'accuracy': dl_accuracy_before, 'precision': dl_precision_before, 'recall': dl_recall_before, 'f1': dl_f1_before}
}

from sklearn.utils import resample

#Perform bootstraping to calculate confidence intervals
def bootstrap_metric(estimator, X, y, metric, n_iterations=1000, alpha=0.95, **kwargs):
    metrics = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y, random_state=np.random.randint(10000))
        estimator.fit(X_resampled, y_resampled)
        y_pred = estimator.predict(X_test)
        metric_value = metric(y_test, y_pred, **kwargs)
        metrics.append(metric_value)
    lower = np.percentile(metrics, (1 - alpha) / 2 * 100)
    upper = np.percentile(metrics, (alpha + (1 - alpha) / 2) * 100)
    return lower, upper

# Random Forest without SMOTE
accuracy_rf_ci = bootstrap_metric(rf_before, X_train_before, y_train_before, accuracy_score)
precision_rf_ci = bootstrap_metric(rf_before, X_train_before, y_train_before, lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted', zero_division=1))
recall_rf_ci = bootstrap_metric(rf_before, X_train_before, y_train_before, lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted', zero_division=1))
f1_rf_ci = bootstrap_metric(rf_before, X_train_before, y_train_before, lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted', zero_division=1))

# SVM without SMOTE
accuracy_svm_ci = bootstrap_metric(svm_before, X_train_before, y_train_before, accuracy_score)
precision_svm_ci = bootstrap_metric(svm_before, X_train_before, y_train_before, lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted', zero_division=1))
recall_svm_ci = bootstrap_metric(svm_before, X_train_before, y_train_before, lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted', zero_division=1))
f1_svm_ci = bootstrap_metric(svm_before, X_train_before, y_train_before, lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted', zero_division=1))

# Deep Learning without SMOTE
accuracy_dl_ci = bootstrap_metric(dl_model_before, X_train_before, y_train_before, accuracy_score)
precision_dl_ci = bootstrap_metric(dl_model_before, X_train_before, y_train_before, lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted', zero_division=1))
recall_dl_ci = bootstrap_metric(dl_model_before, X_train_before, y_train_before, lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted', zero_division=1))
f1_dl_ci = bootstrap_metric(dl_model_before, X_train_before, y_train_before, lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted', zero_division=1))

# Random Forest with SMOTE
accuracy_rf_smote_ci = bootstrap_metric(rf_after, X_train, y_train, accuracy_score)
precision_rf_smote_ci = bootstrap_metric(rf_after, X_train, y_train, lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted', zero_division=1))
recall_rf_smote_ci = bootstrap_metric(rf_after, X_train, y_train, lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted', zero_division=1))
f1_rf_smote_ci = bootstrap_metric(rf_after, X_train, y_train, lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted', zero_division=1))

# SVM with SMOTE
accuracy_svm_smote_ci = bootstrap_metric(svm_after, X_train, y_train, accuracy_score)
precision_svm_smote_ci = bootstrap_metric(svm_after, X_train, y_train, lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted', zero_division=1))
recall_svm_smote_ci = bootstrap_metric(svm_after, X_train, y_train, lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted', zero_division=1))
f1_svm_smote_ci = bootstrap_metric(svm_after, X_train, y_train, lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted', zero_division=1))

# Deep Learning with SMOTE
accuracy_dl_smote_ci = bootstrap_metric(dl_model_after, X_train, y_train, accuracy_score)
precision_dl_smote_ci = bootstrap_metric(dl_model_after, X_train, y_train, lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted', zero_division=1))
recall_dl_smote_ci = bootstrap_metric(dl_model_after, X_train, y_train, lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted', zero_division=1))
f1_dl_smote_ci = bootstrap_metric(dl_model_after, X_train, y_train, lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted', zero_division=1))

print(f"Random Forest Confidence Interval without SMOTE")
print(f"Accuracy 95% CI: {accuracy_rf_ci}")
print(f"Precision 95% CI: {precision_rf_ci}")
print(f"Recall 95% CI: {recall_rf_ci}")
print(f"F1 Score 95% CI: {f1_rf_ci}")

print(f"Random Forest Confidence Interval with SMOTE")
print(f"Accuracy 95% CI: {accuracy_rf_smote_ci}")
print(f"Precision 95% CI: {precision_rf_smote_ci}")
print(f"Recall 95% CI: {recall_rf_smote_ci}")
print(f"F1 Score 95% CI: {f1_rf_smote_ci}")

print(f"SVM Confidence Interval without SMOTE")
print(f"Accuracy 95% CI: {accuracy_svm_ci}")
print(f"Precision 95% CI: {precision_svm_ci}")
print(f"Recall 95% CI: {recall_svm_ci}")
print(f"F1 Score 95% CI: {f1_svm_ci}")

print(f"SVM Confidence Interval with SMOTE")
print(f"Accuracy 95% CI: {accuracy_svm_smote_ci}")
print(f"Precision 95% CI: {precision_svm_smote_ci}")
print(f"Recall 95% CI: {recall_svm_smote_ci}")
print(f"F1 Score 95% CI: {f1_svm_smote_ci}")

print(f"Deep Learning Confidence Interval without SMOTE")
print(f"Accuracy 95% CI: {accuracy_dl_ci}")
print(f"Precision 95% CI: {precision_dl_ci}")
print(f"Recall 95% CI: {recall_dl_ci}")
print(f"F1 Score 95% CI: {f1_dl_ci}")

print(f"Deep Learning Confidence Interval with SMOTE")
print(f"Accuracy 95% CI: {accuracy_dl_smote_ci}")
print(f"Precision 95% CI: {precision_dl_smote_ci}")
print(f"Recall 95% CI: {recall_dl_smote_ci}")
print(f"F1 Score 95% CI: {f1_dl_smote_ci}")

from scipy.stats import ttest_ind, wilcoxon

# Function to collect performance metrics for statistical comparison
def collect_scores(estimator, X, y, X_test, y_test, n_iterations=100):
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    f1_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y, random_state=np.random.randint(10000))
        estimator.fit(X_resampled, y_resampled)
        y_pred = estimator.predict(X_test)
        accuracy_scores.append(accuracy_score(y_test, y_pred))
        precision_scores.append(precision_score(y_test, y_pred, average='weighted', zero_division=1))
        recall_scores.append(recall_score(y_test, y_pred, average='weighted', zero_division=1))
        f1_scores.append(f1_score(y_test, y_pred, average='weighted', zero_division=1))
    return accuracy_scores, precision_scores, recall_scores, f1_scores

# Collect scores for Random Forest with and without SMOTE
scores_rf_with_smote = collect_scores(rf_after, X_resampled, y_resampled, X_test, y_test)
scores_rf_without_smote = collect_scores(rf_before, X_train, y_train, X_test, y_test)

# Collect scores for SVM with and without SMOTE
scores_svm_with_smote = collect_scores(svm_before, X_resampled, y_resampled, X_test, y_test)
scores_svm_without_smote = collect_scores(svm_after, X_train, y_train, X_test, y_test)

# Collect scores for Deep Learning with and without SMOTE
def collect_dl_scores(model, X, y, X_test, y_test, n_iterations=100):
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    f1_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y, random_state=np.random.randint(10000))
        model.fit(X_resampled, y_resampled)
        y_pred = (model.predict(X_test) > 0.5).astype("int32")
        accuracy_scores.append(accuracy_score(y_test, y_pred))
        precision_scores.append(precision_score(y_test, y_pred, average='weighted', zero_division=1))
        recall_scores.append(recall_score(y_test, y_pred, average='weighted', zero_division=1))
        f1_scores.append(f1_score(y_test, y_pred, average='weighted', zero_division=1))
    return accuracy_scores, precision_scores, recall_scores, f1_scores

# Collect DL scores with and without SMOTE
scores_dl_with_smote = collect_dl_scores(dl_model_after, X_resampled, y_resampled, X_test, y_test)
scores_dl_without_smote = collect_dl_scores(dl_model_before, X_train, y_train, X_test, y_test)

# Extract specific metric scores from the tuples returned
accuracy_rf_with_smote, precision_rf_with_smote, recall_rf_with_smote, f1_rf_with_smote = scores_rf_with_smote
accuracy_rf_without_smote, precision_rf_without_smote, recall_rf_without_smote, f1_rf_without_smote = scores_rf_without_smote

accuracy_svm_with_smote, precision_svm_with_smote, recall_svm_with_smote, f1_svm_with_smote = scores_svm_with_smote
accuracy_svm_without_smote, precision_svm_without_smote, recall_svm_without_smote, f1_svm_without_smote = scores_svm_without_smote

accuracy_dl_with_smote, precision_dl_with_smote, recall_dl_with_smote, f1_dl_with_smote = scores_dl_with_smote
accuracy_dl_without_smote, precision_dl_without_smote, recall_dl_without_smote, f1_dl_without_smote = scores_dl_without_smote

# Perform statistical tests for Random Forest
t_stat_rf_accuracy, p_value_rf_accuracy = ttest_ind(accuracy_rf_with_smote, accuracy_rf_without_smote)
stat_rf_accuracy, p_rf_accuracy = wilcoxon(accuracy_rf_with_smote, accuracy_rf_without_smote)

t_stat_rf_precision, p_value_rf_precision = ttest_ind(precision_rf_with_smote, precision_rf_without_smote)
stat_rf_precision, p_rf_precision = wilcoxon(precision_rf_with_smote, precision_rf_without_smote)

t_stat_rf_recall, p_value_rf_recall = ttest_ind(recall_rf_with_smote, recall_rf_without_smote)
stat_rf_recall, p_rf_recall = wilcoxon(recall_rf_with_smote, recall_rf_without_smote)

t_stat_rf_f1, p_value_rf_f1 = ttest_ind(f1_rf_with_smote, f1_rf_without_smote)
stat_rf_f1, p_rf_f1 = wilcoxon(f1_rf_with_smote, f1_rf_without_smote)

print(f"Random Forest Accuracy T-test p-value: {p_value_rf_accuracy}, Wilcoxon p-value: {p_rf_accuracy}")
print(f"Random Forest Precision T-test p-value: {p_value_rf_precision}, Wilcoxon p-value: {p_rf_precision}")
print(f"Random Forest Recall T-test p-value: {p_value_rf_recall}, Wilcoxon p-value: {p_rf_recall}")
print(f"Random Forest F1 Score T-test p-value: {p_value_rf_f1}, Wilcoxon p-value: {p_rf_f1}")

# Perform statistical tests for SVM
t_stat_svm_accuracy, p_value_svm_accuracy = ttest_ind(accuracy_svm_with_smote, accuracy_svm_without_smote)
stat_svm_accuracy, p_svm_accuracy = wilcoxon(accuracy_svm_with_smote, accuracy_svm_without_smote)

t_stat_svm_precision, p_value_svm_precision = ttest_ind(precision_svm_with_smote, precision_svm_without_smote)
stat_svm_precision, p_svm_precision = wilcoxon(precision_svm_with_smote, precision_svm_without_smote)

t_stat_svm_recall, p_value_svm_recall = ttest_ind(recall_svm_with_smote, recall_svm_without_smote)
stat_svm_recall, p_svm_recall = wilcoxon(recall_svm_with_smote, recall_svm_without_smote)

t_stat_svm_f1, p_value_svm_f1 = ttest_ind(f1_svm_with_smote, f1_svm_without_smote)
stat_svm_f1, p_svm_f1 = wilcoxon(f1_svm_with_smote, f1_svm_without_smote)

print(f"SVM Accuracy T-test p-value: {p_value_svm_accuracy}, Wilcoxon p-value: {p_svm_accuracy}")
print(f"SVM Precision T-test p-value: {p_value_svm_precision}, Wilcoxon p-value: {p_svm_precision}")
print(f"SVM Recall T-test p-value: {p_value_svm_recall}, Wilcoxon p-value: {p_svm_recall}")
print(f"SVM F1 Score T-test p-value: {p_value_svm_f1}, Wilcoxon p-value: {p_svm_f1}")

def safe_wilcoxon(x, y):
    if np.all(np.array(x) == np.array(y)):
        return (0, 1)  # No difference between samples
    else:
        return wilcoxon(x, y)

# Perform statistical tests for Deep Learning
t_stat_dl_accuracy, p_value_dl_accuracy = ttest_ind(accuracy_dl_with_smote, accuracy_dl_without_smote)
stat_dl_accuracy, p_dl_accuracy = safe_wilcoxon(accuracy_dl_with_smote, accuracy_dl_without_smote)

t_stat_dl_precision, p_value_dl_precision = ttest_ind(precision_dl_with_smote, precision_dl_without_smote)
stat_dl_precision, p_dl_precision = safe_wilcoxon(precision_dl_with_smote, precision_dl_without_smote)

t_stat_dl_recall, p_value_dl_recall = ttest_ind(recall_dl_with_smote, recall_dl_without_smote)
stat_dl_recall, p_dl_recall = safe_wilcoxon(recall_dl_with_smote, recall_dl_without_smote)

t_stat_dl_f1, p_value_dl_f1 = ttest_ind(f1_dl_with_smote, f1_dl_without_smote)
stat_dl_f1, p_dl_f1 = safe_wilcoxon(f1_dl_with_smote, f1_dl_without_smote)

print(f"Deep Learning Accuracy T-test p-value: {p_value_dl_accuracy}, Wilcoxon p-value: {p_dl_accuracy}")
print(f"Deep Learning Precision T-test p-value: {p_value_dl_precision}, Wilcoxon p-value: {p_dl_precision}")
print(f"Deep Learning Recall T-test p-value: {p_value_dl_recall}, Wilcoxon p-value: {p_dl_recall}")
print(f"Deep Learning F1 T-test p-value: {p_value_dl_f1}, Wilcoxon p-value: {p_dl_f1}")

import pandas as pd

# Summary DataFrame for confidence intervals
ci_summary = pd.DataFrame({
    'Model': ['RF_with_SMOTE', 'RF_without_SMOTE', 'SVM_with_SMOTE', 'SVM_without_SMOTE', 'DL_with_SMOTE', 'DL_without_SMOTE'],
    'Accuracy_CI': [accuracy_rf_smote_ci, accuracy_rf_ci, accuracy_svm_smote_ci, accuracy_svm_ci, accuracy_dl_smote_ci, accuracy_dl_ci],
    'Precision_CI': [precision_rf_smote_ci, precision_rf_ci, precision_svm_smote_ci, precision_svm_ci, precision_dl_smote_ci, precision_dl_ci],
    'Recall_CI': [recall_rf_smote_ci, recall_rf_ci, recall_svm_smote_ci, recall_svm_ci, recall_dl_smote_ci, recall_dl_ci],
    'F1_CI': [f1_rf_smote_ci, f1_rf_ci, f1_svm_smote_ci, f1_svm_ci, f1_dl_smote_ci, f1_dl_ci]
})

print(ci_summary)

# Perform statistical tests for Random Forest
t_stat_rf, p_value_rf = ttest_ind(scores_rf_with_smote, scores_rf_without_smote)
stat_rf, p_rf = safe_wilcoxon(scores_rf_with_smote, scores_rf_without_smote)

# Perform statistical tests for SVM
t_stat_svm, p_value_svm = ttest_ind(scores_svm_with_smote, scores_svm_without_smote)
stat_svm, p_svm = safe_wilcoxon(scores_svm_with_smote, scores_svm_without_smote)

# Perform statistical tests for Deep Learning (already done)
# Deep Learning statistical tests already covered previously

print(f"Random Forest T-test p-value: {p_value_rf}, Wilcoxon p-value: {p_rf}")
print(f"SVM T-test p-value: {p_value_svm}, Wilcoxon p-value: {p_svm}")
print(f"Deep Learning T-test p-value: {p_value_dl_accuracy}, Wilcoxon p-value: {p_dl_accuracy}")

# Create a summary DataFrame for statistical test results
stat_summary = pd.DataFrame({
    'Model': ['Random Forest', 'SVM', 'Deep Learning'],
    'T-test p-value': [p_value_rf, p_value_svm, p_value_dl_accuracy],
    'Wilcoxon p-value': [p_rf, p_svm, p_dl_accuracy]
})

print(stat_summary)